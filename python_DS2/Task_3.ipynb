{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Урок 3. Построение модели классификации.\n",
    "\n",
    "### Задание 1.\n",
    "\n",
    "*Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* micro - учитывает общие показатели без учета классов, скоре всего, будет хорош если нет дисбаланса классов или нет требований к важности определенного класса при предсказании.\n",
    "* macro - рассчитывается как средне, будет более правильным при ярко выраженном дисбалансе классов, насколько я понял, используется чаще всего. Т.е. если есть дисбаланс и нам важно предсказание маленького класса - берем macro.\n",
    "* weighted - взвешенное среднее по классам, что-то среднее между micro и macro, учитывает показатели каждого класса, но при дисбалансе снижает вес маленького класса. Получается он выгоден если нам важны предсказание больших классов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Задание 2.\n",
    "\n",
    "*В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?*\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все эти модели относятс к градиентному бустингу, но имеют некоторые особенности:\n",
    "\n",
    "* xgboost - отличается подходом к построению дерева решений. В xgboost разбиение выбирается не по энтропии, а по  similarity score (\"похожести\") для этого высчитывается отклонения каждого эксперимента попавшего в разбиение. \n",
    "* lightgbm - при построении дерева пытается оптимизировать скорость, для этого используются приемы: \n",
    " * leaf-wise growth - дерево растет в глубину только на листьях с высокой энтропией;\n",
    " * GOSS - каждое следующее дерево, в рамках бустинга, обучается не на всех данных, а в основном на тех, которые дают наибольшую ошибку (отбирается доля данных с наибольшей ошибкой и небольшая доля нормальных данных);\n",
    " * EFB - фичи, которые можно объединять без потери информации, - объединяются.\n",
    "* CatBoost:\n",
    " * для ускорения строит симметричное дерево;\n",
    " * борятся с переобучением высоким уровнем рандома выборки и большей объективностью при оценке моделей (ошибка считается по данным, на которых модель не обучалась);\n",
    " * улучшена поддержка категориальных признаков.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
