{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "data = boston[\"data\"]\n",
    "feature_names = boston[\"feature_names\"]\n",
    "target = boston[\"target\"]\n",
    "\n",
    "X = pd.DataFrame(data, columns=feature_names)\n",
    "y = pd.DataFrame(target, columns=[\"price\"])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6712577134237363"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_true = y_test, y_pred = y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=1000, max_depth=12, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8589184343153571"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train.values[:, 0])\n",
    "y_pred_forest = model.predict(X_test)\n",
    "r2_score(y_true = y_test, y_pred = y_pred_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае лучше работает модель Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestRegressor in module sklearn.ensemble._forest:\n",
      "\n",
      "class RandomForestRegressor(ForestRegressor)\n",
      " |  RandomForestRegressor(n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest regressor.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of classifying\n",
      " |  decision trees on various sub-samples of the dataset and uses averaging\n",
      " |  to improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"mse\", \"mae\"}, default=\"mse\"\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"mse\" for the mean squared error, which is equal to variance\n",
      " |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      " |      absolute error.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, default=None\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      whether to use out-of-bag samples to estimate\n",
      " |      the R^2 on unseen data.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int or RandomState, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0, 1)`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : DecisionTreeRegressor\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeRegressor\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_prediction_ : ndarray of shape (n_samples,)\n",
      " |      Prediction computed with out-of-bag estimate on the training set.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor, ExtraTreesRegressor\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  The default value ``max_features=\"auto\"`` uses ``n_features``\n",
      " |  rather than ``n_features / 3``. The latter was originally suggested in\n",
      " |  [1], whereas the former was more recently justified empirically in [2].\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      " |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      " |  ...                        random_state=0, shuffle=False)\n",
      " |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      " |  >>> regr.fit(X, y)\n",
      " |  RandomForestRegressor(...)\n",
      " |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      " |  [-8.32987858]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestRegressor\n",
      " |      ForestRegressor\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestRegressor:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict regression target for X.\n",
      " |      \n",
      " |      The predicted regression target of an input sample is computed as the\n",
      " |      mean predicted regression targets of the trees in the forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix or a list of generic objects instead,\n",
      " |          shape = (n_samples, n_samples_fitted),\n",
      " |          where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The R2 score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.03002634, 0.00079703, 0.00502605, 0.00102708, 0.01934644,\n",
       "       0.52240922, 0.0125729 , 0.0498377 , 0.00486121, 0.01346689,\n",
       "       0.01780966, 0.013228  , 0.30959149])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(feature_names)\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сумма равна 1, а наибольшую важность имеют RM и LSTAT.\n",
    "Статус окружения влияет даже больше чем количество комнат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.998273\n",
       "1    0.001727\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./creditcard.csv', sep=',')\n",
    "df['Class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>-0.371407</td>\n",
       "      <td>1.341262</td>\n",
       "      <td>0.359894</td>\n",
       "      <td>-0.358091</td>\n",
       "      <td>-0.137134</td>\n",
       "      <td>0.517617</td>\n",
       "      <td>0.401726</td>\n",
       "      <td>-0.058133</td>\n",
       "      <td>0.068653</td>\n",
       "      <td>-0.033194</td>\n",
       "      <td>0.084968</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>-0.099254</td>\n",
       "      <td>-1.416907</td>\n",
       "      <td>-0.153826</td>\n",
       "      <td>-0.751063</td>\n",
       "      <td>0.167372</td>\n",
       "      <td>0.050144</td>\n",
       "      <td>-0.443587</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>-0.611987</td>\n",
       "      <td>-0.045575</td>\n",
       "      <td>-0.219633</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>1.249376</td>\n",
       "      <td>-0.619468</td>\n",
       "      <td>0.291474</td>\n",
       "      <td>1.757964</td>\n",
       "      <td>-1.323865</td>\n",
       "      <td>0.686133</td>\n",
       "      <td>-0.076127</td>\n",
       "      <td>-1.222127</td>\n",
       "      <td>-0.358222</td>\n",
       "      <td>0.324505</td>\n",
       "      <td>-0.156742</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>-0.410430</td>\n",
       "      <td>-0.705117</td>\n",
       "      <td>-0.110452</td>\n",
       "      <td>-0.286254</td>\n",
       "      <td>0.074355</td>\n",
       "      <td>-0.328783</td>\n",
       "      <td>-0.210077</td>\n",
       "      <td>-0.499768</td>\n",
       "      <td>0.118765</td>\n",
       "      <td>0.570328</td>\n",
       "      <td>0.052736</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>-0.366846</td>\n",
       "      <td>1.017614</td>\n",
       "      <td>0.836390</td>\n",
       "      <td>1.006844</td>\n",
       "      <td>-0.443523</td>\n",
       "      <td>0.150219</td>\n",
       "      <td>0.739453</td>\n",
       "      <td>-0.540980</td>\n",
       "      <td>0.476677</td>\n",
       "      <td>0.451773</td>\n",
       "      <td>0.203711</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "5  0.260314 -0.568671 -0.371407  1.341262  0.359894 -0.358091 -0.137134   \n",
       "6  0.081213  0.464960 -0.099254 -1.416907 -0.153826 -0.751063  0.167372   \n",
       "7 -3.807864  0.615375  1.249376 -0.619468  0.291474  1.757964 -1.323865   \n",
       "8  0.851084 -0.392048 -0.410430 -0.705117 -0.110452 -0.286254  0.074355   \n",
       "9  0.069539 -0.736727 -0.366846  1.017614  0.836390  1.006844 -0.443523   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "5  0.517617  0.401726 -0.058133  0.068653 -0.033194  0.084968 -0.208254   \n",
       "6  0.050144 -0.443587  0.002821 -0.611987 -0.045575 -0.219633 -0.167716   \n",
       "7  0.686133 -0.076127 -1.222127 -0.358222  0.324505 -0.156742  1.943465   \n",
       "8 -0.328783 -0.210077 -0.499768  0.118765  0.570328  0.052736 -0.073425   \n",
       "9  0.150219  0.739453 -0.540980  0.476677  0.451773  0.203711 -0.246914   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "5 -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080   \n",
       "6 -0.270710 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168   \n",
       "7 -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   \n",
       "8 -0.268092 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   \n",
       "9 -0.633753 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  \n",
       "5    3.67      0  \n",
       "6    4.99      0  \n",
       "7   40.80      0  \n",
       "8   93.20      0  \n",
       "9    3.68      0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:,:'Amount']\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 199364 entries, 222925 to 271\n",
      "Data columns (total 30 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    199364 non-null  float64\n",
      " 1   V1      199364 non-null  float64\n",
      " 2   V2      199364 non-null  float64\n",
      " 3   V3      199364 non-null  float64\n",
      " 4   V4      199364 non-null  float64\n",
      " 5   V5      199364 non-null  float64\n",
      " 6   V6      199364 non-null  float64\n",
      " 7   V7      199364 non-null  float64\n",
      " 8   V8      199364 non-null  float64\n",
      " 9   V9      199364 non-null  float64\n",
      " 10  V10     199364 non-null  float64\n",
      " 11  V11     199364 non-null  float64\n",
      " 12  V12     199364 non-null  float64\n",
      " 13  V13     199364 non-null  float64\n",
      " 14  V14     199364 non-null  float64\n",
      " 15  V15     199364 non-null  float64\n",
      " 16  V16     199364 non-null  float64\n",
      " 17  V17     199364 non-null  float64\n",
      " 18  V18     199364 non-null  float64\n",
      " 19  V19     199364 non-null  float64\n",
      " 20  V20     199364 non-null  float64\n",
      " 21  V21     199364 non-null  float64\n",
      " 22  V22     199364 non-null  float64\n",
      " 23  V23     199364 non-null  float64\n",
      " 24  V24     199364 non-null  float64\n",
      " 25  V25     199364 non-null  float64\n",
      " 26  V26     199364 non-null  float64\n",
      " 27  V27     199364 non-null  float64\n",
      " 28  V28     199364 non-null  float64\n",
      " 29  Amount  199364 non-null  float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 47.2 MB\n",
      "199364\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 85443 entries, 262922 to 198375\n",
      "Data columns (total 30 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Time    85443 non-null  float64\n",
      " 1   V1      85443 non-null  float64\n",
      " 2   V2      85443 non-null  float64\n",
      " 3   V3      85443 non-null  float64\n",
      " 4   V4      85443 non-null  float64\n",
      " 5   V5      85443 non-null  float64\n",
      " 6   V6      85443 non-null  float64\n",
      " 7   V7      85443 non-null  float64\n",
      " 8   V8      85443 non-null  float64\n",
      " 9   V9      85443 non-null  float64\n",
      " 10  V10     85443 non-null  float64\n",
      " 11  V11     85443 non-null  float64\n",
      " 12  V12     85443 non-null  float64\n",
      " 13  V13     85443 non-null  float64\n",
      " 14  V14     85443 non-null  float64\n",
      " 15  V15     85443 non-null  float64\n",
      " 16  V16     85443 non-null  float64\n",
      " 17  V17     85443 non-null  float64\n",
      " 18  V18     85443 non-null  float64\n",
      " 19  V19     85443 non-null  float64\n",
      " 20  V20     85443 non-null  float64\n",
      " 21  V21     85443 non-null  float64\n",
      " 22  V22     85443 non-null  float64\n",
      " 23  V23     85443 non-null  float64\n",
      " 24  V24     85443 non-null  float64\n",
      " 25  V25     85443 non-null  float64\n",
      " 26  V26     85443 non-null  float64\n",
      " 27  V27     85443 non-null  float64\n",
      " 28  V28     85443 non-null  float64\n",
      " 29  Amount  85443 non-null  float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 20.2 MB\n",
      "85443\n"
     ]
    }
   ],
   "source": [
    "X_train.info()\n",
    "print(y_train.count())\n",
    "X_test.info()\n",
    "print(y_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'n_estimators': [10, 15],\n",
    "    'max_features': np.arange(3, 5),\n",
    "    'max_depth': np.arange(4, 7),\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=100),\n",
    "    param_grid=parameters,\n",
    "    scoring='roc_auc',\n",
    "    cv=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=100),\n",
       "             param_grid={'max_depth': array([4, 5, 6]),\n",
       "                         'max_features': array([3, 4]),\n",
       "                         'n_estimators': [10, 15]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6, 'max_features': 3, 'n_estimators': 15}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99070828e-01 9.29171738e-04]\n",
      " [9.99704794e-01 2.95206364e-04]\n",
      " [9.99717846e-01 2.82154033e-04]\n",
      " [9.99717846e-01 2.82154033e-04]\n",
      " [9.99717846e-01 2.82154033e-04]\n",
      " [9.99717846e-01 2.82154033e-04]\n",
      " [9.99717846e-01 2.82154033e-04]\n",
      " [9.99717846e-01 2.82154033e-04]\n",
      " [9.99717846e-01 2.82154033e-04]\n",
      " [9.99717846e-01 2.82154033e-04]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = clf.predict_proba(X_test)\n",
    "\n",
    "print(y_pred_proba[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = y_pred_proba[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAFNCAYAAABmLCa9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwVdZrv8c+TkA3IAiEQSIiAgmyyiSgICq6oIGC7BJRA23Od3u3XTM90z3TfnrnTfe+dme6Z2+3Y3Y7TgyFhVURUGkVtURFFREEEFBpFDHsIS0L25Dz3j1OBEJOck6VSZ3ner1deOXVOnVNPkeTLr6p+9fuJqmKMMaZlMV4XYIwxoc6C0hhjArCgNMaYACwojTEmAAtKY4wJwILSGGMCsKA0xpgALCiN60TkCxGpFJHzInJcRPJFpGeTdaaIyOsiUiYi50TkRREZ2WSdFBH5tYh86XzWAWe5T9fukYk2FpSmq8xW1Z7AOGA88HcNL4jIZOAV4HlgADAY+AjYIiJDnHXigT8Bo4CZQAowBSgBJrlVtIh0c+uzTfiwoDRdSlWPAxvxB2aDfwUKVPU3qlqmqqdV9afAVuAfnXXygBxgnqruVVWfqp5U1Z+r6obmtiUio0TkVRE5LSInROTvnefzReQXjdabLiKHGy1/ISI/EpFdQLmI/FRE1jT57N+IyGPO41QR+W8ROSYiR0TkFyIS28F/KhNCLChNlxKRbOAO4ICz3B1/y/CZZlZ/GrjVeXwL8LKqng9yO8nAa8DL+FupV+BvkQZrPnAXkAYUAneKSIrz2bHA/cAKZ92lQJ2zjfHAbcBftGFbJsRZUJqusk5EyoAi4CTwD87zvfH/Hh5r5j3HgIbzj+ktrNOSWcBxVf03Va1yWqrvteH9j6lqkapWquoh4ENgrvPaTUCFqm4VkX74g/8HqlquqieB/wfktmFbJsRZUJquMldVk4HpwHAuBuAZwAf0b+Y9/YFTzuOSFtZpyUDgs3ZV6lfUZHkF/lYmwAIutiYvA+KAYyJyVkTOAv8J9O3Atk2IsaA0XUpV3wTygV85y+XAu8B9zax+PxcPl18DbheRHkFuqgi4vIXXyoHujZYzmyu1yfIzwHTn1ME8LgZlEVAN9FHVNOcrRVVHBVmnCQMWlMYLvwZuFZGGCzo/BhaJyPdFJFlEejkXWyYD/8tZpxB/KD0rIsNFJEZE0kXk70Xkzma2sR7IFJEfiEiC87nXOq/txH/OsbeIZAI/CFSwqhYDbwBPAQdV9RPn+WP4r9j/m9N9KUZELheRG9vx72JClAWl6XJO6BQA/9NZfhu4HbgH/3nIQ/gvikxV1T8761Tjv6DzKfAqUApsw38I/5Vzj6pahv9C0GzgOPBnYIbzciH+7kdf4A+51UGWvsKpYUWT5/OAeGAv/lMJa2jbaQIT4sQG7jXGmNZZi9IYYwKwoDTGmAAsKI0xJgALSmOMCcCC0hhjAgi7kVH69OmjgwYN8roMY0yE+eCDD06pakZzr4VdUA4aNIjt27d7XYYxJsKIyKGWXrNDb2OMCcCC0hhjArCgNMaYACwojTEmAAtKY4wJwILSGGMCsKA0xpgAXAtKEVkiIidFZHcLr4uIPObMzbxLRCa4VYsxxnSEmy3KfPzzL7fkDmCo8/UI8HsXazHGmHZz7c4cVX1LRAa1ssoc/HM5K7BVRNJEpL8ztL4xxgRFVSmtquPU+WqKy6opLq2krKqOGSP60T81qVO24eUtjFlcOtPdYee5rwSliDyCv9VJTk5OlxRnjPFOQ/gVl1Vz6rzzVVZN8flqTpXVXHiuuKyaU+U11NT5AIjBx/T4z6nTGLJ7PxARQSnNPNfsvBSq+iTwJMDEiRNt7gpjwpCqUlpZ5w+7hpC7EII1F54/VVbNqfM11NT7vvIZsTFCeo94+vRMoE9yAlf0TaZPcjwZPRPonRTL0R2bKDl6linTb2HS4PROq93LoDyMf+7lBtnAUY9qMca0Q+PwuyT4LizXtCn8MpITGNoo/DKSE/yh2DOBPj3j6dU9npiYr7axVJXly5dTcvQQs2bN4uqrr+7U/fQyKF8Avisiq4BrgXN2ftIY710MvyqKmx7mnr8YfsVl1ZQECL+GoGsp/DKSE0hLims2/NpCRBg2bBijR49m3Lhxgd/QRq4FpYisBKYDfUTkMPAPQByAqj4BbADuBA4AFcDX3arFmGinqpyrrHUCzjnMLWu+9ddS+HWLEdJ7xl8IuWH9ki+09BrCr+F7Z4RfMKqrqzl16hRZWVlMmjTJte24edV7foDXFfiOW9s3JtI1Dr+TDUFX1nzr79T5amrrv3p6v3H4ZSQncGXmpeGX4ZwL7MrwC1ZVVRXLli2jpKSERx99lMTERNe2FXYD9xoTyRrCr7jhCq8Tfpe2AGv8h73lLYef/2KHPwAbws/f2ou/EH4ZPRNIDbHwC1ZlZSWFhYWcOHGC++67z9WQBAtKY1ynqpytcA57m7nIUdzoym+w4Tc8M/lCS++S1l8Yh1+wysvLKSws5NSpU+Tm5jJ06FDXt2lBaUw7+HyNz/k1av010+3l1Plq6nxfDb+4WCG9R8KFixwjMlMuhN8lrb8oCL+2eO+99ygpKWH+/PlcfvnlXbJNC0pjHD6fctYJv4aW3iWtv0YXPkrO17QYfheu6DYKv4vn+i5e+U1NikPEwq+tpk+fzqhRo+jXr1+XbdOC0kS0QOHXuPUXbPiN7J9ysW+fE4IZziGxhZ87zp07x/r167n77rtJTk7u0pAEC0oThhqHX3HZpd1bmnZ6Dib8+qUkMmrAxfC72NXFwi8UnDlzhoKCAiorKykrKyM5ObnLa7CgNCHB51POVNRccphb3MK9vafLWw6/hkPchvC79M6OhAsXPVKSuln4hYGSkhIKCgqora0lLy+PAQMGeFKHBaVxTdPwu9D6a3xvr/NcSXkN9c2EX3xsDH16xl8Iv9EDUi9c+b2k9WfhF3FKSkrIz8/H5/ORl5dHZmamZ7VYUJo2aQi/5lp6jfv9BRt+/VMTuSrrYvhdcnubhV9US0pKom/fvtx+++307dvX01osKA0+n3K6ouaS7iyXtP4anfs73Ur4NXRpaQi/huU+TW5vS0m08DMtO3XqFL169aJ79+4sXLjQ63IAC8qIcaiknI17jvP+F2fwNRNkTdXU+y4cErcYft1inH588QxITWRsduqFDs59mtzeZuFnOsPRo0cpLCxk9OjR3HXXXV6Xc4EFZZhSVfYcLeWVPcfZuOcE+06UATAkowfd42MDvj82JoastEvDLyM58ZIWoIWf6UpFRUUsX76cpKQkrr/+eq/LuYQFZRiqqKlj0ZJtvP/FGWIEJg7qzf+cNZLbRvZjYO/uXpdnTJsdOnSIFStW0LNnT/Ly8khNTfW6pEtYUIYZn0/5waqdfHDoDH9z+5XkXjOQ9J4JXpdlTLvV1dXx7LPPkpKSQl5enif9JAOxoAwze4+V8sreE/zN7VfynRlXeF2OMR3WrVs3cnNzSUlJoWfPnl6X0yw3p6s1nazep7z/xWkArsoKrUMTY9pq//79bNmyBYABAwaEbEiCtSjDQmVNPWs+PMyStw9y8FQ52b2SuKJv6P5SGRPIJ598wpo1a8jMzOTaa6+lW7fQjqLQri7KFZdVU/DuFyzbeogzFbWMzU7lP+aP547RmXSLtYMBE552797N2rVrycrK4sEHHwz5kAQLypDzybFS9h0v493PSnhuxxFqfT5uGdGP/zFtCNcM6mXddUxY27VrF+vWrWPgwIEsWLCAhITwuBBpQRkiKmrq+NXG/Tz1zkFUITEuhvuvyebh6wczJMMOs01kqKurY9CgQeTm5hIfH+91OUGzoHTJnz45wer3i4Jef++xUg6fqeSh63JYPGUQfVMSSUmMc7FCY7pOaWkpKSkpTJgwgfHjx4fdkZEFpQvq6n387Pk9VNTU0S8luEmPBqQm8av7xnLdkHSXqzOma23dupU//elPPPzww/Tv3z/sQhIsKDuNz+e/pfBPn57g1b0nOHK2kv/Km8itI7t2JGZjQsk777zDq6++yogRIzwfAagjLCiDVFPn4/VPT35lYvjaOh/bDp5m076TnCyrRgTGDUzjZ7NGcvPw8P3FMKaj3nrrLTZt2sSoUaOYN28esbGBxyAIVRaUQVq38wh/u2ZXs68lJ3TjhiszuOnKvky/MsNuKTRRb9++fWzatIkxY8YwZ84cYmLCuzubBaXjyNlK/vmlT6mt8zX7+r4TZaR1j2PNNycDF8+xiEBO7+7EWb9GYy4YNmwYd999N2PHjg37kAQLygv+9eVP2bj7OIP79Gj29fjYGPImD+KKvqF3w74xoUBVefvttxkzZgypqamMHz/e65I6jQUlsO94GS98dJRv3ng5P5o53OtyjAk7qspLL73E+++/D8C0adM8rqhzRV1Qbjt4mpNlVZc8t2pbET3ju/GXNwzxqCpjwpeq8uKLL7Jjxw6mTJnC1KlTvS6p00VVUJ46X839//lus6/99a3DSOsePncKGBMKfD4fL7zwAh999BHTpk1jxowZYdlPMpCoCsojZyoB+PmcUZd07I6JEQanN39u0hjTstraWk6ePMn06dO58cYbvS7HNVEVlCdK/Yfc4wb2Ymg/uyhjTHvV19ejqiQkJPDwww+HxQhAHRHZe+f42fO7KXj30IXlfqnWz9GY9qqrq2PNmjX4fD7mz58f8SEJURCUZytqLoTk928eyoDURPomB3f/tTHmUrW1tTz99NMcOHCAO++8MyLPRzYn4oPyt5sOAHDbyH781a3DPK7GmPBVW1vLqlWr+Pzzz5k9ezYTJkzwuqQuE/FBuf3QGQB+kxs5nV+N8cJzzz3HwYMHmTNnDuPGjfO6nC4V8UHZI96/i0nx4XtDvjGh4IYbbmDkyJGMHj3a61K6XPjfhBmAolx9WS+vyzAmLFVWVvLBBx8AkJmZGZUhCVHQolRtPISFMSZYFRUVFBYWUlxczKBBg0hPj95BpaMjKC0pjWmT8vJyCgoKKCkpITc3N6pDElw+9BaRmSKyT0QOiMiPm3k9VUReFJGPRGSPiHy9s2tQFLE2pTFBKysrIz8/n9OnT7NgwQKuuOIKr0vynGtBKSKxwG+BO4CRwHwRGdlkte8Ae1V1LDAd+DcR6dQbrq1FaUzbHD58mLKyMh588EGGDLGBYsDdQ+9JwAFV/RxARFYBc4C9jdZRIFn8vVZ7AqeBus4s4vCZSgakWQdzYwKpr68nNjaWESNGcNlll9G9e3evSwoZbh56ZwGN52s97DzX2OPACOAo8DHwqKo2P8R4O/VPTaSkvKYzP9KYiHPmzBl+//vfc+CA/wYNC8lLuRmUzR3wapPl24GdwABgHPC4iKR85YNEHhGR7SKyvbi4uG1FCGQGOWWsMdGopKSEp556ioqKCnr0sFG0muNmUB4GBjZazsbfcmzs68Ba9TsAHAS+MsS4qj6pqhNVdWJGRoZrBRsTbYqLi8nPz6e+vp5FixbRv39/r0sKSW4G5fvAUBEZ7FygyQVeaLLOl8DNACLSD7gS+NzFmowxjtLSUvLz8wFYvHgx/frZHPQtce1ijqrWich3gY1ALLBEVfeIyDed158Afg7ki8jH+A/Vf6Sqp9yqyRhzUXJyMhMmTGDcuHFR308yEFc7nKvqBmBDk+eeaPT4KHCbmzUYYy515MgRkpKS6N27NzfffLPX5YSFyL/Xu+nlI2OiWFFREQUFBbz44otelxJWIv4WRrAO58YAHDp0iOXLl5OcnMy8efO8LiesREVQGhPtPv/8c1auXElaWhp5eXkkJ9ucUW1hQWlMhFNVNm/eTO/evcnLy7O+ku1gQWlMBFNVRIQHHngAn89nd9y0U8RfzDEmWu3du5fly5dTW1tLYmKihWQHWFAaE4F2797NmjVrqKmpwefr1OETopIdehsTYT766COef/55cnJymD9/PgkJNo99R0V8UFo3ShNNdu3axbp16xg8eDC5ubnEx3fq8K5RK+KDErARzk3UyMzM5KqrrmL27NnExcV5XU7EsHOUxkSAQ4cOoar07duXe+65x0Kyk1lQGhPmtmzZQn5+Prt37/a6lIgVFYfexkSqN998kzfeeIPRo0czcmTTKalMZ7GgNCYMqSqbNm1i8+bNjBkzhjlz5hATYweIbrGgNCYMnTx5krfffpvx48cza9YsC0mXRXxQqo2zZiJQv379+MY3vsGAAQMQGx7LdVHx35D9HplIoKq89NJLfPrppwBkZWVZSHaRqAhKY8Kdz+fjxRdfZNu2bRw5csTrcqJOxB96GxPufD4fzz//PLt27eKGG25g+vTpXpcUdSwojQlhPp+PtWvXsmfPHmbMmMENN9zgdUlRyYLSmBAmIvTs2ZNbbrmF66+/3utyopYFpTEhqK6ujrKyMnr16sXtt99uF208FvEXc6xzkAk3tbW1rFq1iqeeeoqamhoLyRAQ8UFpTDipqalh5cqVfPbZZ8yYMcOGSQsRduhtTIiorq5mxYoVFBUVMW/ePMaMGeN1ScYRdItSRGzqNmNctGnTJoqKirjnnnssJENMwBaliEwB/gD0BHJEZCzwl6r6bbeLMyaazJgxg2HDhjFkyBCvSzFNBNOi/H/A7UAJgKp+BFhnLmM6QUVFBevXr6empoaEhAQLyRAV1KG3qhY1earehVqMiSrl5eUsXbqUjz76iBMnTnhdjmlFMBdzipzDbxWReOD7wCfultV5bPAgE4rKysooKCjg7NmzzJ8/n4EDB3pdkmlFMC3KbwLfAbKAw8A4IKzOT1o/NBNKSktLyc/P59y5czz00EN2uB0GgmlRXqmqDzZ+QkSuB7a4U5Ixka22thYRYeHChdaSDBPBtCj/I8jnjDGtKC8vR1VJT0/n29/+toVkGGmxRSkik4EpQIaI/FWjl1KAWLcLMyaSnDp1ioKCAsaNG8dNN91kUzeEmdYOvePx953sBiQ3er4UuNfNooyJJMXFxSxduhSAUaNGeVyNaY8Wg1JV3wTeFJF8VT3UhTUZEzFOnDhBQUEBMTEx5OXlkZGR4XVJph2CuZhTISK/BEYBiQ1PqupNrlXViax3kPFKbW0ty5YtIzY2lkWLFpGenu51SaadggnK5cBqYBb+rkKLgGI3i+ps1jnIeCEuLo7Zs2eTkZFBr169vC7HdEAwZ5TTVfW/gVpVfVNVHwauc7kuY8LWl19+yZ49ewAYNmyYhWQECKZFWet8PyYidwFHgWz3SjImfH3xxResWLGCtLQ0hg8fTmysdRCJBMG0KH8hIqnAXwM/xD+S0A+C+XARmSki+0TkgIj8uIV1povIThHZIyJvBl25MSHm888/Z/ny5aSlpbFw4UILyQgSsEWpquudh+eAGXDhzpxWiUgs8FvgVvy3Pr4vIi+o6t5G66QBvwNmquqXItK37btgjPf+/Oc/s3r1atLT08nLy6NHDxu+NZK02KIUkVgRmS8iPxSR0c5zs0TkHeDxID57EnBAVT9X1RpgFTCnyToLgLWq+iWAqp5s114Y47GioiL69u3LokWLLCQjUGstyv8GBgLbgMdE5BAwGfixqq4L4rOzgMbDsx0Grm2yzjAgTkTewN+p/TeqWhBk7cGx4YOMi2pqaoiPj2fGjBlMmzaNuLg4r0syLmgtKCcCY1TVJyKJwCngClU9HuRnN9crp2lqdQOuBm4GkoB3RWSrqu6/5INEHgEeAcjJyQly843f3+a3GBPQxx9/zCuvvMLixYtJT0+3kIxgrV3MqVFVH4CqVgH72xCS4G9BNr7rPxv/FfOm67ysquWqegp4Cxjb9INU9UlVnaiqE+3OBhMKdu7cydq1a0lPTyc5OTnwG0xYa61FOVxEdjmPBbjcWRZAVTXQ7EfvA0NFZDBwBMjFf06yseeBx0WkG/57y6/FP/WEMSHrgw8+YP369QwZMoTc3FxrSUaB1oJyREc+WFXrROS7wEb8ow0tUdU9IvJN5/UnVPUTEXkZ2AX4gD+o6u6ObNcYN3366aesX7+eK664ggceeIBu3WzG52jQ2qAYHR4IQ1U3ABuaPPdEk+VfAr/s6LaM6QqXX345N954I1OnTrWQjCI2KJ4xQdi5cydVVVXExcUxffp0C8koE/FBaZ2DTEeoKm+++SbPP/8827Zt87oc45GgglJEkkTkSreLcYv1DjLtoaps2rSJN954g3HjxjF16lSvSzIeCRiUIjIb2Am87CyPE5EX3C7MGC+pKq+++iqbN29mwoQJ3H333TZ9QxQL5if/j/hvRzwLoKo7gUHulWSM9yorK9mzZw/XXHMNs2bNsimPo1wwZ6TrVPWc/aKYaKDOLa/du3fnkUceoXv37haSJqig3C0iC4BYERkKfB94x92yjOl6Pp+PF198kdjYWO666y4b3MJcEMyh9/fwz5dTDazAP9xaUONRGhMufD4f69atY+fOnfTs2dPrckyICaZFeaWq/gT4idvFuMEGDzKB1NfX89xzz7Fnzx5uuukmpk2b5nVJJsQE06L8dxH5VER+LiJhOSmxnWMyrVm3bh179uzh1ltvtZA0zQpmhPMZIpIJ3A88KSIpwGpV/YXr1RnTBcaMGcPAgQOZNGmS16WYEBVUxzBVPa6qj+GfrnYn8DNXqzLGZbW1tRw4cACAoUOHWkiaVgXT4XyEiPyjiOzGPwXEO9gsjCaM1dTUsGLFClauXMmZM2e8LseEgWAu5jwFrARuU9WmA+8aE1aqq6tZsWIFRUVFzJ071+bcNkEJ5hzldV1RiDFuq6qqYvny5Rw5coR77rmH0aNHe12SCRMtBqWIPK2q94vIx1w6CE+wI5yHBLXxg4xj7969HD16lPvuu48RIzo0LrWJMq21KB91vs/qikLcZJ2DopuqIiKMHz+enJwc+vTp43VJJsy0eDFHVY85D7+tqocafwHf7pryjOmY8+fPs3TpUo4fP46IWEiadgmme9CtzTx3R2cXYkxnKysrY+nSpRw9epTKykqvyzFhrLVzlN/C33Ic0mg2RoBkYIvbhRnTEefOnaOgoIDz58/z4IMPctlll3ldkgljrZ2jXAG8BPxf4MeNni9T1dOuVmVMB5SWlpKfn09lZSUPPfQQAwcODPwmY1rRWlCqqn4hIt9p+oKI9LawNKGqe/fuZGdnM3nyZAYMGOB1OSYCBGpRzgI+wN89qPHFYwWGuFiXMW1WUlJC9+7dSUpK4mtf+5rX5ZgI0tq83rOc74O7rpzOZ8OsRYeTJ09SUFDAgAEDWLBggdflmAgTzL3e14tID+fxQyLy7yKS435pncdGWYtsx48fZ+nSpYgIt912m9flmAgUTPeg3wMVIjIW+FvgEFDoalXGBOno0aMsXbqUbt26sXjxYusnaVwRTFDWqX/GpTnAb1T1N/i7CBnjKVVl/fr1JCYmsnjxYtLT070uyUSoYEYPKhORvwMWAtNEJBaIc7csYwITEe6//35EhNTUVK/LMREsmBblA/gnFntYVY8DWcAvXa3KmFYcPHiQP/7xj6gqaWlpFpLGdQGD0gnH5UCqiMwCqlS1wPXKjGnGZ599xooVKzh06BBVVVVel2OiRDBXve8HtgH34Z835z0RudftwjqLdQ+KHH/+859ZuXIl6enpLFq0iKSkJK9LMlEimHOUPwGuUdWTACKSAbwGrHGzsM5l/YPC3aeffsozzzxDv379WLhwoYWk6VLBBGVMQ0g6SghyUjJjOkt8fDw5OTk88MADJCYmel2OiTLBBOXLIrIR/7w54L+4s8G9koy56PTp0/Tu3ZshQ4YwePBgm6PdeCKYizl/A/wnMAYYCzypqj9yuzBjduzYweOPP87+/fsBLCSNZ1obj3Io8CvgcuBj4IeqeqSrCjPR7YMPPmD9+vUXWpLGeKm1FuUSYD3wNfwjCP1Hl1Rkot62bdtYv349Q4cOZf78+cTF2f0NxlutnaNMVtX/ch7vE5EPu6Kgzma9g8LL0aNHeemllxg+fDj33nsvsbGxXpdkTKtBmSgi47nYtyap8bKqhk1w2qmt8DFgwADuv/9+hg0bZiFpQkZrQXkM+PdGy8cbLStwk1tFmeiiqmzZsoXBgweTlZVlc26bkNPawL0zurIQE51Ulddff523336bSZMmkZWV5XVJxnyFqx3HRWSmiOwTkQMi8uNW1rtGROrD6dZI03GqyiuvvMLbb7/N1VdfzcyZM70uyZhmuRaUznBsv8U/B/hIYL6IjGxhvX8BNrpViwk9qspLL73E1q1bmTRpEnfddZf1kzQhy80W5STggKp+rqo1wCr8g/829T3gWeBkM6+ZCOXz+SgtLWXy5MnMnDnTQtKEtIC3MIr/N/hBYIiq/pMzX06mqm4L8NYsoKjR8mHg2iafnQXMw39h6Jq2FB4steGDQorP56O6upqkpKQLg+5aSJpQF0yL8nfAZGC+s1yG/5A6kOZ++5um1q+BH6lqfasfJPKIiGwXke3FxcVBbDpwIabr+Xw+1q1bR35+PrW1tcTExFhImrAQTFBeq6rfAaoAVPUMEB/E+w4DAxstZwNHm6wzEVglIl8A9wK/E5G5TT9IVZ9U1YmqOjEjIyOITZtQU19fz7PPPsvHH3/MVVddZXfbmLASzOhBtc4FF4UL41H6gnjf+8BQERkMHAFygUsmXG48Z7iI5APrVXVdcKWbcFFXV8eaNWvYt28ft912G5MnT/a6JGPaJJigfAx4DugrIv8bf8vvp4HepKp1IvJd/FezY4ElqrpHRL7pvP5E+8s24eSVV15h37593HHHHUyaNMnrcoxps4BBqarLReQD4Gb8p/vmquonwXy4qm6gydiVLQWkqi4O5jNN+Jk6dSrZ2dmMGTPG61KMaZdg5szJASqAF4EXgHLnOWNaVFNTw+bNm/H5fKSkpFhImrAWzKH3H/GfnxQgERgM7ANGuViXCWPV1dUsX76cw4cPc9lll5GTY/+vmvAWzKH3VY2XRWQC8JeuVeQC64HSdaqqqli2bBnHjh3j3nvvtZA0ESGYFuUlVPVDEXGlc7gJbxUVFSxbtowTJ05w3333MXz4cK9LMqZTBHNnzl81WowBJgBt7/VtIt7p06c5e/Ysubm5DB061OtyjOk0wbQokxs9rsN/zjEcD2gAABJBSURBVPJZd8ox4aiuro5u3bqRnZ3No48+SkJCgtclGdOpWg1Kp6N5T2cmRmO+orS0lIKCAq677jomTpxoIWkiUmuzMHZzOo1P6MqCTPg4d+4cS5cupby8nL59+3pdjjGuaa1FuQ3/+cidIvIC8AxQ3vCiqq51ubZOYYMHuePMmTMUFBRQWVnJwoULyc7O9rokY1wTzDnK3kAJ/qHQGvpTKhAWQQkgNn5Qp6quriY/P5+amhry8vIYMGCA1yUZ46rWgrKvc8V7NxcDsoG106JYQkICU6dOZeDAgWRmZnpdjjGuay0oY4GeBDeupIkCJ0+epKqqipycHK65xrrSmujR6nS1qvpPXVaJCWnHjx+noKCAHj168K1vfYuYGFfnpTMmpLQWlHZizwBw9OhRCgsLiY+PZ/78+RaSJuq0FpQ3d1kVJmQVFRWxfPlykpKSWLRoEWlpaV6XZEyXazEoVfV0VxbiFrXTqR2yY8cOevToQV5eHqmpqV6XY4wn2jwoRjiy0YPaTlUREe666y6qqqro0aOH1yUZ4xk72WS+4sCBAzz55JOcP3+e2NhYC0kT9SwozSX279/PqlWrAOyijTGOqDj0NsH55JNPWLNmDZmZmTz00EMkJSV5XZIxIcGC0gD+luQzzzxDVlYWDz74IImJiV6XZEzIsKA0AGRlZTF27FhmzpxpQ6UZ00TEn4Sy0YNa99lnn1FfX0+PHj2YM2eOhaQxzYj4oATrHtSS7du3s2zZMt59912vSzEmpNmhd5R67733ePnllxk2bBjXXXed1+UYE9IsKKPQli1beO211xg+fDj33nsvsbGxXpdkTEizoIwyZWVlbN68mVGjRjFv3jwLSWOCYEEZZZKTk/nGN75Benq6dSg3Jkj2lxIFVJXXXnuNd955B4CMjAwLSWPawP5aIpyqsnHjRrZs2cKZM2dQ6y9lTJtF/KF3NMeCqrJhwwa2b9/Otddey+23345YXylj2izigxKicxZGVWX9+vV8+OGHTJkyhVtuucVC0ph2ioqgjEYiQmZmJtOmTWPGjBkWksZ0gAVlhPH5fJw8eZLMzEybKdGYTmIXcyJIfX09a9asYcmSJZSWlnpdjjERw4IyQtTV1fHMM8/wySefcNNNN5GSkuJ1ScZEDDv0jgC1tbU8/fTTHDhwgDvvvNMOuY3pZBEflNHQb3D79u0cOHCA2bNnM2HCBK/LMSbiRHxQAkR676Brr72W/v37M2jQIK9LMSYi2TnKMFVdXc3atWspLS0lJibGQtIYF7kalCIyU0T2icgBEflxM68/KCK7nK93RGSsm/VEisrKSgoLC9mzZw/Hjx/3uhxjIp5rh94iEgv8FrgVOAy8LyIvqOreRqsdBG5U1TMicgfwJHCtWzVFgoqKCgoLCzl58iT33Xcfw4YN87okYyKem+coJwEHVPVzABFZBcwBLgSlqr7TaP2tQLaL9YS98vJyCgoKKCkpITc3l6FDh3pdkjFRwc1D7yygqNHyYee5lnwDeMnFesKeiBAfH8+CBQssJI3pQm62KJu71txsXx0RmYE/KKe28PojwCMAOTk5bSoiEjoHnT9/nsTERLp3787DDz9s920b08XcbFEeBgY2Ws4GjjZdSUTGAH8A5qhqSXMfpKpPqupEVZ2YkZHR5kLCOVbOnj3LkiVLeOGFFwAsJI3xgJtB+T4wVEQGi0g8kAu80HgFEckB1gILVXW/i7WEpTNnzpCfn09FRQWTJk3yuhxjopZrh96qWici3wU2ArHAElXdIyLfdF5/AvgZkA78zmkp1anqRLdqCiclJSUsXbqUuro6Fi1aRP/+/b0uyZio5eqdOaq6AdjQ5LknGj3+C+Av3KwhHKkqq1evpr6+nkWLFtGvXz+vSzImqkXHLYxhRkSYO3cucXFxtOecrDGmc9ktjCHk2LFjF2ZKHDBggIWkMSEi8oMyTPoHHTlyhIKCArZt20ZVVZXX5RhjGomKQ+9Q71JTVFTEsmXL6NGjB3l5eSQmJnpdkjGmkagIylD2xRdfsGLFCpKTk1m0aJGNTG5MCLKg9NjZs2dJS0tj4cKFJCcne12OMaYZFpQeqaqqIjExkXHjxnHVVVcRGxvrdUnGmBZE/sWcELRv3z5+/etf8+WXXwJYSBoT4iwou9jevXt5+umnSU9Pt+4/xoSJiD/0DqXeQbt372bt2rVkZ2ezYMECu7ptTJiI+KCE0Bg9qKioiLVr15KTk8P8+fNJSEjwuiRjTJCiIihDQXZ2NrfeeitXX3018fHxXpdjjGkDO0fpsp07d3L27FlEhMmTJ1tIGhOGLChdtHXrVp5//nm2bNnidSnGmA6wQ2+XbNmyhddee40RI0Ywc+ZMr8sxxnSABaUL3nrrLTZt2sTo0aOZN28eMTHWcDcmnEV8UKp2bQehuro69u3bx5gxY5gzZ46FpDERIOKDEqArBg9SVXw+H926dSMvL4+4uDgLSWMihP0ldwJVZePGjaxcuZL6+noSEhIsJI2JIPbX3EGqyoYNG3jvvffo06ePBaQxESgqDr3d4vP5WL9+PTt27GDKlCnccsstIT9IsDGm7SwoO+CVV15hx44d3HDDDUyfPt1C0pgIZUHZAePHjyclJYUpU6Z4XYoxxkURf0KtszsH1dfX8/HHH6Oq9OvXz0LSmCgQFS3Kzjogrqur45lnnmH//v306tWL7OzsTvpkY0woi4qg7Ay1tbWsXr2azz77jDvvvNNC0pgoYkEZhJqaGlatWsXBgweZPXs2EyZM8LokY0wXsqAMQlFREYcOHWLu3LmMHTvW63KMMV3MgrIVqoqIcPnll/P973+f1NRUr0syxngg4q96t1dlZSVPPfUU+/fvB7CQNCaKRXyLsj2DB1VUVFBYWEhxcXGXjz5kjAk9ER+UQJvumDl//jyFhYWcPn2a3NxcrrjiChcrM8aEg6gIymBVVVWxdOlSzp49y/z58xkyZIjXJRljQoAFZSMJCQkMHTqUK6+8kssuu8zrcowxIcKCEjh79iw+n4/evXtz2223eV2OMSbERP1V79OnT5Ofn8/TTz9tF26MMc2K6qA8deoU+fn51NTUMGfOHBsmzRjTrKg99C4uLmbp0qUALFq0iH79+nlckTEmVEV8UGoLA629/vrriAh5eXlkZGR0cVXGmHAS8UEJzQ+zNnfuXMrLy+ndu3eX12OMCS9RdY7y8OHDrF69mtraWhISEiwkjTFBcTUoRWSmiOwTkQMi8uNmXhcRecx5fZeIuDZ+2ZdffklhYSEnTpygsrLSrc0YYyKQa0EpIrHAb4E7gJHAfBEZ2WS1O4ChztcjwO/dqCW2/BTLli0jOTmZxYsXk5KS4sZmjDERys0W5STggKp+rqo1wCpgTpN15gAF6rcVSBOR/p1ZRK/6s/Q4vJW0tDQLSWNMu7gZlFlAUaPlw85zbV0HEXlERLaLyPbi4uI2FVEt8dQl9WbRokX07NmzTe81xhhw96p3cxebm/bVCWYdVPVJ4EmAiRMntun2mccW30j3+Fh69OjelrcZY8wFbgblYWBgo+Vs4Gg71umQKzOTO/PjjDFRyM1D7/eBoSIyWETigVzghSbrvADkOVe/rwPOqeoxF2syxpg2c61Fqap1IvJdYCMQCyxR1T0i8k3n9SeADcCdwAGgAvi6W/UYY0x7uXpnjqpuwB+GjZ97otFjBb7jZg3GGNNRUXVnjjHGtIcFpTHGBGBBaYwxAVhQGmNMABaUxhgTgAWlMcYEYEFpjDEBSLjNPCgixcChNr6tD3DKhXK8ECn7Ein7AbYvoaqt+3KZqjY7L0zYBWV7iMh2VZ3odR2dIVL2JVL2A2xfQlVn7osdehtjTAAWlMYYE0C0BOWTXhfQiSJlXyJlP8D2JVR12r5ExTlKY4zpiGhpURpjTLtFVFCG0vS4HRHEfjzo1L9LRN4RkbFe1BmMQPvSaL1rRKReRO7tyvraIph9EZHpIrJTRPaIyJtdXWMwgvj9ShWRF0XkI2c/QnacWBFZIiInRWR3C693zt+8qkbEF/7BgT8DhgDxwEfAyCbr3Am8hH+unuuA97yuu537MQXo5Ty+IxT3I9h9abTe6/jHLr3X67o78HNJA/YCOc5yX6/rbud+/D3wL87jDOA0EO917S3szw3ABGB3C693yt98JLUoQ2J63E4QcD9U9R1VPeMsbsU/11AoCuZnAvA94FngZFcW10bB7MsCYK2qfgmgqqG4P8HshwLJIiJAT/xBWde1ZQZHVd/CX19LOuVvPpKCstOmx/VYW2v8Bv7/MUNRwH0RkSxgHvAEoS2Yn8swoJeIvCEiH4hIXpdVF7xg9uNxYAT+if4+Bh5VVV/XlNfpOuVv3tWpILpYp02P67GgaxSRGfiDcqqrFbVfMPvya+BHqlrvb8CErGD2pRtwNXAzkAS8KyJbVXW/28W1QTD7cTuwE7gJuBx4VUQ2q2qp28W5oFP+5iMpKENietxOEFSNIjIG+ANwh6qWdFFtbRXMvkwEVjkh2Qe4U0TqVHVd15QYtGB/v06pajlQLiJvAWOBUArKYPbj68A/q/8k3wEROQgMB7Z1TYmdqnP+5r0+GduJJ3W7AZ8Dg7l4knpUk3Xu4tITu9u8rrud+5GDf+bKKV7X29F9abJ+PqF7MSeYn8sI4E/Out2B3cBor2tvx378HvhH53E/4AjQx+vaW9mnQbR8MadT/uYjpkWpETI9bpD78TMgHfid0xKr0xAcyCDIfQkLweyLqn4iIi8DuwAf8AdVbbbbileC/Jn8HMgXkY/xB8yPVDUkRxQSkZXAdKCPiBwG/gGIg879m7c7c4wxJoBIuuptjDGusKA0xpgALCiNMSYAC0pjjAnAgtIYYwKwoDRBcUb22dnoa1Ar657vhO3li8hBZ1sfisjkdnzGH0RkpPP475u89k5Ha3Q+p+HfZbcz4k5agPXHicidnbFt03Wse5AJioicV9Wenb1uK5+RD6xX1TUichvwK1Ud04HP63BNgT5XRJYC+1X1f7ey/mJgoqp+t7NrMe6xFqVpFxHpKSJ/clp7H4vIV0YFEpH+IvJWoxbXNOf520TkXee9z4hIoAB7C7jCee9fOZ+1W0R+4DzXQ0T+6IyfuFtEHnCef0NEJorIPwNJTh3LndfOO99XN27hOS3Zr4lIrIj8UkTed8Yx/Msg/lnexRlwQUQmiX+s0B3O9ytFJB74J+ABp5YHnNqXONvZ0dy/owkBXt9+ZF/h8QXU4x8oYSfwHP5b4VKc1/rgv/Oh4QjlvPP9r4GfOI9jgWRn3beAHs7zPwJ+1sz28nFuZwTuA97DP+DEx0AP/MN/7QHGA18D/qvRe1Od72/gb71dqKnROg01zgOWOo/j8Y80kwQ8AvzUeT4B2A4MbqbO84327xlgprOcAnRzHt8CPOs8Xgw83uj9/wd4yHmchv++8B5e/7zt69KviLmF0biuUlXHNSyISBzwf0TkBvy362Xhvy/4eKP3vA8scdZdp6o7ReRGYCSwxbn9Mh5/S6w5vxSRnwLF+EdJuhl4Tv2DTiAia4FpwMvAr0TkX/Afrm9uw369BDwmIgnATOAtVa10DvfHyMUR11OBocDBJu9PEpGd+O83/gB4tdH6S0VkKP7RauJa2P5twN0i8kNnORH/vfyftGEfjMssKE17PYh/9OurVbVWRL7A/0d+gaq+5QTpXUChiPwSOAO8qqrzg9jG36jqmoYFEbmluZVUdb+IXI3/nt7/KyKvqOo/BbMTqlolIm/gH1rsAWBlw+aA76nqxgAfUamq40QkFVgPfAd4DP/90ptUdZ5z4euNFt4vwNdUdV8w9Rpv2DlK016pwEknJGcAlzVdQUQuc9b5L+C/8Q/ZvxW4XkQazjl2F5FhQW7zLWCu854e+A+bN4vIAKBCVZcBv3K201St07Jtzir8gyVMwz9YBM73bzW8R0SGOdtslqqeA74P/NB5Tyr+UXfAf7jdoAz/KYgGG4HvidO8FpHxLW3DeMeC0rTXcmCiiGzH37r8tJl1pgM7RWQH/vOIv1HVYvzBsVJEduEPzuHBbFBVP8R/7nIb/nOWf1DVHcBVwDbnEPgnwC+aefuTwK6GizlNvIJ/7pXX1D89AvjH+twLfCj+iav+kwBHYE4tHwG5wL/ib91uwX/+ssEmYGTDxRz8Lc84p7bdzrIJMdY9yBhjArAWpTHGBGBBaYwxAVhQGmNMABaUxhgTgAWlMcYEYEFpjDEBWFAaY0wAFpTGGBPA/weRO/Vx+buRpwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 6, 4\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = 5, 5\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='dashed')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
